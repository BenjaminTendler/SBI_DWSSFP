{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Import Packages\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "##\n",
    "#SBI Specific Packages\n",
    "import torch\n",
    "from sbi import analysis as analysis\n",
    "from sbi import utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Define Path to Code database\n",
    "DirPath = '/PATH/TO/bin/'\n",
    "\n",
    "#Define Path for Posterior Object\n",
    "InputPath = '/PATH/TO/Posterior.pkl'\n",
    "\n",
    "#Define Path to Example Data\n",
    "DataPath = '/PATH/TO/ExampleData/'\n",
    "\n",
    "#Define Output Path\n",
    "OutputPath = '/PATH/TO/Output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Import Custom Functions\n",
    "sys.path.append(DirPath)\n",
    "from ImportData import *\n",
    "from FreedAnalytical import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Load Posterior\n",
    "with open(InputPath, \"rb\") as handle:\n",
    "    posterior = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Load Data\n",
    "    # Data - DW-SSFP Data (4D)\n",
    "    # T1Map - T1 Map (ms, 3D)\n",
    "    # T2Map - T2 Map (ms, 3D)\n",
    "    # B1Map - B1 Map (Normalised, 3D)\n",
    "    # Mask - Mask (Binary, 3D)\n",
    "    # noisefloor - noisefloor estimate (1xn)\n",
    "    # bvecs - bvectors (3xn)\n",
    "    # FlipAngles - Flip Angles (degrees) (1xn)\n",
    "    # tau - Diffusion Gradient Duration (seconds) (1xn), \n",
    "    # G - Diffusion Gradient Duration (G/cm - Equivalent to mT/m Divided by 10) (1xn)\n",
    "    # TRs - Repetition Times (seconds) (1xn)\n",
    "    # b0s - Array Defining b0 Locations (b0 = 1, dwi = 0) (1xn)\n",
    "Data, T1Map, T2Map, B1Map, Mask, noisefloor, bvecs, FlipAngles, tau, G, TRs, b0s = ImportDataDWSSFP(DataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Estimate Mean Noisefloor\n",
    "Noisefloor_Mean = np.mean(noisefloor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Normalise Data by S0 (Estimate S0 per Flip Angle) & Perform Noisefloor Correction \n",
    "\n",
    "##\n",
    "#Define unique locations\n",
    "Values, Index = np.unique(FlipAngles, return_index=True)\n",
    "\n",
    "##\n",
    "#Normalised Data for Each Flip angle\n",
    "for idx, k in enumerate(Index):\n",
    "    #Calculate Theoretical Signal Amplitude\n",
    "    b0 = FreedDWSSFP(G[k], tau[k], TRs[k], FlipAngles[k]*B1Map.data, 0, T1Map.data, T2Map.data)\n",
    "    #Identify S0 (Incorporating Noisefloor Contribution)\n",
    "    S0 = np.abs(np.mean(Data.data[:,:,:,(b0s == 1) & (FlipAngles == Values[idx])],axis = 3)**2-Noisefloor_Mean**2)**0.5/b0*Mask.data\n",
    "    #Divide by S0 & Remove NoiseFloor\n",
    "    Data.data[:,:,:,FlipAngles == Values[idx]] = np.abs(Data.data[:,:,:,FlipAngles == Values[idx]]**2-Noisefloor_Mean**2)**0.5/S0[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Perform Posterior Estimation (NPE)\n",
    "\n",
    "##\n",
    "#Define Number of Samples\n",
    "Samples = 100\n",
    "\n",
    "##\n",
    "#Initialise Output Array (Tensor Estimates & Voxels where Posterior Couldn't be Evaluated)\n",
    "Tensor = np.zeros((*T1Map.data.shape,Samples,6))\n",
    "TensorErr = np.zeros(T1Map.data.shape)\n",
    "\n",
    "##\n",
    "#Perform Posterior Estimation for all Voxels in Dataset\n",
    "for k in range(Data.data.shape[0]):\n",
    "    for l in range(Data.data.shape[1]):\n",
    "        for m in range(Data.data.shape[2]):\n",
    "            if Mask.data[k,l,m] == 0:\n",
    "                continue\n",
    "            #Concatenate Data \n",
    "            DataVec = torch.from_numpy(np.concatenate((Data.data[k,l,m,:],[B1Map.data[k,l,m]/100],[T1Map.data[k,l,m]/100000],[T2Map.data[k,l,m]/10000])))[np.newaxis,:]\n",
    "            #Perform Posterior Estimation\n",
    "            Tensor[k,l,m,:,:] = posterior.sample((Samples,), x=DataVec, show_progress_bars = False)\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Save Mean of Tensor Estimates\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,0],axis=3),Data.aff),''.join([OutputPath, 'D11_Mean.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,1],axis=3),Data.aff),''.join([OutputPath, 'D22_Mean.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,2],axis=3),Data.aff),''.join([OutputPath, 'D33_Mean.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,3],axis=3),Data.aff),''.join([OutputPath, 'D12_Mean.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,4],axis=3),Data.aff),''.join([OutputPath, 'D13_Mean.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,5],axis=3),Data.aff),''.join([OutputPath, 'D23_Mean.nii.gz']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Save Full Posterior Outputs of Tensor Components\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,0],Data.aff),''.join([OutputPath, 'D11_Samples.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,1],Data.aff),''.join([OutputPath, 'D22_Samples.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,2],Data.aff),''.join([OutputPath, 'D33_Samples.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,3],Data.aff),''.join([OutputPath, 'D12_Samples.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,4],Data.aff),''.join([OutputPath, 'D13_Samples.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,5],Data.aff),''.join([OutputPath, 'D23_Samples.nii.gz']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Convert the Diffusivity Outputs to Eigenvalue/Eigenvector/FA Outputs in FSL via\n",
    "#fslmerge -t dti D11_Mean.nii.gz D12_Mean.nii.gz D13_Mean.nii.gz D22_Mean.nii.gz D23_Mean.nii.gz D33_Mean.nii.gz\n",
    "#fslmaths dti.nii.gz -tensor_decomp dti"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
