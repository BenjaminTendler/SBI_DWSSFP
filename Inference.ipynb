{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Import Packages\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import sys\n",
    "import pickle\n",
    "from interruptingcow import timeout\n",
    "\n",
    "##\n",
    "#SBI Specific Packages\n",
    "import torch\n",
    "from sbi import analysis as analysis\n",
    "from sbi import utils as utils\n",
    "from sbi.inference import SNPE, NPE, MCMCPosterior, posterior_estimator_based_potential, simulate_for_sbi\n",
    "from sbi.utils import RestrictionEstimator\n",
    "from sbi.utils.user_input_checks import check_sbi_inputs, process_prior, process_simulator\n",
    "from sbi.analysis import conditional_corrcoeff, conditional_pairplot, conditional_potential, pairplot, pairplot\n",
    "from sbi.neural_nets.embedding_nets import FCEmbedding\n",
    "from sbi.neural_nets import posterior_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Import custom functions\n",
    "sys.path.append('YourPath')\n",
    "from ImportData import *\n",
    "from FreedAnalytical import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Load Posterior and inference module\n",
    "with open(\"YourPosterior.pkl\", \"rb\") as handle:\n",
    "    posterior = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Define Data Path and load data\n",
    "DataPath = 'YourDataPath'\n",
    "Data, T1Map, T2Map, B1Map, Mask, noisefloor, bvecs, FlipAngles, tau, G, TRs = ImportDataDWSSFP(DataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Normalise Data by S0\n",
    "\n",
    "##\n",
    "#Initialise Arrays\n",
    "Data.norm = Data.data*0\n",
    "\n",
    "##\n",
    "#Define unique locations\n",
    "Values, Index = np.unique(FlipAngles, return_index=True)\n",
    "\n",
    "##\n",
    "#Estimate normalised data for each flip angle\n",
    "for idx, k in enumerate(Index):\n",
    "    #Calculate theoretical signal amplitude\n",
    "    b0 = FreedDWSSFP(G[k], tau[k], TRs[k], FlipAngles[k]*B1Map.data, 0, T1Map.data, T2Map.data)\n",
    "    #Identify S0 (incorporating noisefloor contribution)\n",
    "    S0 = np.mean((Data.data[:,:,:,(tau == 0) & (FlipAngles == Values[idx])]**2-np.mean(noisefloor[(tau == 0) & (FlipAngles == Values[idx])])**2)**0.5,axis = 3)/b0*Mask.data\n",
    "    #Divide by S0\n",
    "    Data.norm[:,:,:,FlipAngles == Values[idx]] = Data.data[:,:,:,FlipAngles == Values[idx]]/S0[:,:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Perform fitting\n",
    "\n",
    "##\n",
    "#Define Number of samples\n",
    "Samples = 100\n",
    "\n",
    "##\n",
    "#Initialise Output Array\n",
    "Tensor = np.zeros((*T1Map.data.shape,Samples,6))\n",
    "TensorErr = np.zeros(T1Map.data.shape)\n",
    "\n",
    "\n",
    "for k in range(Data.data.shape[0]):\n",
    "    for l in range(Data.data.shape[1]):\n",
    "        for m in range(Data.data.shape[2]):\n",
    "            if Mask.data[k,l,m] == 0:\n",
    "                pass\n",
    "            else:\n",
    "                #Generate Data Vector\n",
    "                try:\n",
    "                    with timeout(5, exception=RuntimeError):\n",
    "                        DataVec = np.concatenate((Data.norm[k,l,m,:],[B1Map.data[k,l,m]/100],[T1Map.data[k,l,m]/100000],[T2Map.data[k,l,m]/10000]))\n",
    "                        Tensor[k,l,m,:,:] = posterior.sample((Samples,), x=torch.from_numpy(DataVec[np.newaxis,:]), show_progress_bars = False)\n",
    "                except:\n",
    "                    TensorErr[k,l,m] = 1\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Output mean tensor estimates\n",
    "OutputPath = 'YourPath'\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,0],axis=3),Data.aff),''.join([OutputPath, 'D11_Mean.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,1],axis=3),Data.aff),''.join([OutputPath, 'D22_Mean.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,2],axis=3),Data.aff),''.join([OutputPath, 'D33_Mean.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,3],axis=3),Data.aff),''.join([OutputPath, 'D12_Mean.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,4],axis=3),Data.aff),''.join([OutputPath, 'D13_Mean.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(np.mean(Tensor[:,:,:,:,5],axis=3),Data.aff),''.join([OutputPath, 'D23_Mean.nii.gz']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Output posterior tensor distributions\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,0],Data.aff),''.join([OutputPath, 'D11_Samples.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,1],Data.aff),''.join([OutputPath, 'D22_Samples.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,2],Data.aff),''.join([OutputPath, 'D33_Samples.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,3],Data.aff),''.join([OutputPath, 'D12_Samples.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,4],Data.aff),''.join([OutputPath, 'D13_Samples.nii.gz']))\n",
    "nib.save(nib.Nifti1Image(Tensor[:,:,:,:,5],Data.aff),''.join([OutputPath, 'D23_Samples.nii.gz']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
