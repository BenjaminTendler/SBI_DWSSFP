{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56638f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Import Packages\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from timeit import default_timer as timer\n",
    "from scipy.stats import iqr\n",
    "\n",
    "##\n",
    "#SBI Specific Packages\n",
    "from sbi import analysis as analysis\n",
    "from sbi import utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Define Path to Code Database\n",
    "DirPath = '/PATH/TO/bin/'\n",
    "\n",
    "#Define Path for Posterior Object\n",
    "InputPath = '/PATH/TO/Posterior.pkl'\n",
    "\n",
    "#Define Path to Example Data\n",
    "DataPath = '/PATH/TO/ExampleData/'\n",
    "\n",
    "#Define Output Path\n",
    "OutputPath = '/PATH/TO/Output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164bd9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Load Posterior\n",
    "with open(InputPath, \"rb\") as handle:\n",
    "    posterior = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Import Custom Functions\n",
    "sys.path.append(DirPath)\n",
    "from ImportData import *\n",
    "from FreedAnalytical import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Load Data\n",
    "    # bvecs - bvectors (3xn)\n",
    "    # FlipAngles - Flip Angles (degrees) (1xn)\n",
    "    # tau - Diffusion Gradient Duration (seconds) (1xn), \n",
    "    # G - Diffusion Gradient Duration (G/cm - Equivalent to mT/m Divided by 10) (1xn)\n",
    "    # TRs - Repetition Times (seconds) (1xn)\n",
    "    # b0s - Array Defining b0 locations (b0 = 1, dwi = 0) (1xn)\n",
    "\n",
    "bvecs, FlipAngles, tau, G, TRs, _ = ImportTextDataDWSSFP(DataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b715776",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Obtain Arbitrary Tensor / Signal Pairs\n",
    "from sbi.inference import SNPE, simulate_for_sbi\n",
    "\n",
    "##\n",
    "#Define Nuber of Simulations\n",
    "nSim = 1000\n",
    "\n",
    "#Define SNR Level (wrt b0)\n",
    "SNR = 20\n",
    "\n",
    "##\n",
    "#Define Prior Bounds\n",
    "PriorLow = [0, 0, 0, -0.00025, -0.00025, -0.00025]\n",
    "PriorHigh = [0.0005, 0.0005, 0.0005, 0.00025, 0.00025, 0.00025]\n",
    "\n",
    "##\n",
    "#Define Prior (Uniform)\n",
    "prior = utils.BoxUniform(low=torch.tensor(PriorLow), high=torch.tensor(PriorHigh))\n",
    "\n",
    "##\n",
    "#Define Range of T1 (ms), T2 (ms) & B1 (Normalised) for Forward Simulator\n",
    "T1Range = [300,1200]\n",
    "T2Range = [20,80]\n",
    "B1Range = [0.2,1.2]\n",
    "\n",
    "##\n",
    "#Define Simulator\n",
    "simulator = lambda theta: FreedDWSSFPTensor_Conditional_SBIWrapper(theta,G,tau,TRs,FlipAngles,bvecs,B1Range,T1Range,T2Range,Cond=True)\n",
    "\n",
    "##\n",
    "#Estimate Tensor / Signal Pairs - Exagerate no Simulations to Account for NaNs (i.e. Invalid Tensors)\n",
    "DArb, SArb = simulate_for_sbi(simulator, prior, nSim*5)\n",
    "\n",
    "##\n",
    "#Add Noise\n",
    "SArb[:,:-3] = np.abs(SArb[:,:-3] + (torch.randn(*SArb[:,:-3].shape)*SArb[:,0][:,np.newaxis]/SNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Obtain nSim Valid Tensor / Signal Pairs for Evaluation\n",
    "Finite_idx = np.squeeze(np.argwhere(np.isfinite(SArb[:,0])))\n",
    "DArb = DArb[Finite_idx[0:nSim],:]\n",
    "SArb = SArb[Finite_idx[0:nSim],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0069e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Perform Time Evaluation - NLLS\n",
    "\n",
    "##\n",
    "#Initialise Matrices\n",
    "TimeNLLS = np.zeros([nSim])\n",
    "\n",
    "##\n",
    "#Define Initial Fitting Point & Bounds (NLLS)\n",
    "Init = np.array([4, 3, 2, -1, 1, -2])*10**-4\n",
    "low = np.array([0, 0, 0, -1, -1, -1])*10**-3\n",
    "high = np.array([1, 1, 1, 1, 1, 1])*10**-3\n",
    "\n",
    "##\n",
    "#Reduce Computational Overhead for NLLS by Converting Inputs Prior to Evaluation\n",
    "B1 = SArb[:,-3].numpy()*1E2\n",
    "T1 = SArb[:,-2].numpy()*1E5\n",
    "T2 = SArb[:,-1].numpy()*1E4\n",
    "SArb_NLLS = SArb[:,:-3].numpy()\n",
    "\n",
    "##\n",
    "#Perform Time Estimation (NLLS)\n",
    "for k in range(nSim):  \n",
    "    start = timer()\n",
    "    tmp, _ = curve_fit(lambda x, *theta: FreedDWSSFPTensor_curve_fit(x, theta, G, tau, TRs, FlipAngles, bvecs, B1[k], T1[k], T2[k]), 1, SArb_NLLS[k,:], p0 = Init, bounds=(low,high), method='trf',maxfev=10**6)\n",
    "    end = timer()\n",
    "    TimeNLLS[k] = (end - start)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bbda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Perform Time Evaluation - NPE\n",
    "\n",
    "##\n",
    "#Define Number of Posterior Samples per Evaluation\n",
    "Samples = np.round(10**np.linspace(0, 4, num=21))\n",
    "\n",
    "##\n",
    "#Initialise Matrices\n",
    "TimeNPE = np.zeros([len(Samples),nSim])\n",
    "\n",
    "##\n",
    "#Perform Time Estimation (NPE)\n",
    "for l in range(len(Samples)):\n",
    "    for k in range(nSim):  \n",
    "        start = timer()\n",
    "        tmp = posterior.sample((int(Samples[l]),), x=SArb[k,:],show_progress_bars=False)\n",
    "        end = timer()\n",
    "        TimeNPE[l,k] = (end - start)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Obtain Data for Plotting\n",
    "MedianTimes = np.array([np.median(TimeNLLS),*np.median(TimeNPE,axis=1)])\n",
    "IQRTimes = np.array([iqr(TimeNLLS),*iqr(TimeNPE,axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae27a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Perform Polynomial Fit\n",
    "\n",
    "##\n",
    "#Create Arrays\n",
    "Off = np.ones(len(Samples))\n",
    "X = np.array(Samples * 1)\n",
    "\n",
    "##\n",
    "#Define Weight Matrix\n",
    "Weights = 1/np.array(IQRTimes[1:])\n",
    "\n",
    "##\n",
    "#Create Matrix\n",
    "A = np.transpose(np.array([[*Off],[*X]]))*Weights[:,np.newaxis]\n",
    "\n",
    "##\n",
    "#Estimate Coefficients\n",
    "Coeff = np.dot(np.linalg.pinv(A),Weights*MedianTimes[1:])\n",
    "\n",
    "##\n",
    "#Reconstruct Line\n",
    "EvRecon = Coeff[0] + X*Coeff[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53564c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Plot Figure\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_size_inches(12,5)\n",
    "xLabels = ['NLLS', 'NPE\\n(1)', 'NPE\\n(10)', 'NPE\\n(100)', 'NPE\\n(1,000)', 'NPE\\n(10,000)']\n",
    "axs[0].bar([0,1,2,3,4,5],[MedianTimes[0],MedianTimes[1],MedianTimes[6],MedianTimes[11],MedianTimes[16],MedianTimes[21]],yerr=[IQRTimes[0],IQRTimes[1],IQRTimes[6],IQRTimes[11],IQRTimes[16],IQRTimes[21]],tick_label=xLabels);axs[0].set_ylim([0,120])\n",
    "axs[0].set_ylabel('Evaluation time (ms)',fontsize=12)\n",
    "axs[1].scatter(np.log10(Samples),MedianTimes[1:],s=60,marker='x',label='Estimated Evaulation Time (Median)');axs[1].set_xlim([0,4]);axs[1].set_ylim([0,120])\n",
    "axs[1].plot(np.log10(Samples),EvRecon,linestyle='--',linewidth=3,label=''.join(['Linear Fit (','{0:.2f}'.format(Coeff[1]*1000),r' $\\mathrm{\\mu} s \\cdot \\mathrm{N}_{\\mathrm{Samples}}$ +', '{0:.2f}'.format(Coeff[0]),' ms)']))\n",
    "axs[1].axhline(MedianTimes[0],linestyle='--',linewidth=3,color='tab:orange',label = 'NLLS Evaluation Time',alpha=0.5)\n",
    "axs[1].set_xticks([0,1,2,3,4])\n",
    "axs[1].set_xticklabels(['1','10','100','1,000','10,000'])\n",
    "axs[1].set_ylabel('Evaluation Time (ms)',fontsize=12)\n",
    "axs[1].set_xlabel(r'Number of Samples (log$_{10}$)',fontsize=12)\n",
    "axs[1].legend(loc='upper left')\n",
    "axs[0].text(-0.2, 1.05, '(a)', transform=axs[0].transAxes, size=22)\n",
    "axs[1].text(-0.2, 1.05, '(b)', transform=axs[1].transAxes, size=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b61632",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Identify no. Samples for NPE versus NLLS Assuming Equivalent Analysis Time\n",
    "noSamplesEqTime = np.round((MedianTimes[0]-Coeff[0])/Coeff[1])\n",
    "\n",
    "print(''.join(['Initialisation Time = ', '{0:.2f}'.format(Coeff[0]),' ms']))\n",
    "print(''.join(['Time Per Posterior Estimate = ', '{0:.2f}'.format(Coeff[1]*1000),' us']))\n",
    "print(''.join(['No. Posterior samples in NLLS Equivalent Time = ', '{0:.0f}'.format(noSamplesEqTime)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Save Figure\n",
    "fig.savefig(''.join([OutputPath,'Figure6.pdf']),dpi=300,format='pdf',bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
